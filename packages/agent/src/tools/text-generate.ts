import { z } from "zod";
import { tool } from "ai";
import { EvalContext } from "../sandbox/sandbox";
import { getEnv } from "../env";
import { getModelName } from "../model";
import debug from "debug";
import { AuthError, LogicError, NetworkError, classifyHttpError, classifyGenericError, isClassifiedError, formatUsageForEvent } from "../errors";

const debugTextGenerate = debug("TextGenerate");

export function makeTextGenerateTool(getContext: () => EvalContext) {
  return tool({
    description: `Generate text based on a prompt using AI.
Takes a prompt and optional temperature/max_chars parameters, returns generated text.
Temperature controls randomness (0=deterministic, 1=very random), default 0.3.`,
    inputSchema: z.object({
      prompt: z
        .string()
        .min(1)
        .describe("Prompt to generate text from"),
      temperature: z
        .number()
        .min(0)
        .max(1)
        .default(0.3)
        .optional()
        .describe("Sampling temperature: 0 is deterministic, 1 is very random (default: 0.3)"),
      max_chars: z
        .number()
        .min(100)
        .max(10000)
        .default(1500)
        .optional()
        .describe("Maximum number of characters to generate (default: 1500)"),
    }),
    outputSchema: z.object({
      text: z.string().describe("Generated text"),
    }),
    execute: async (input) => {
      const { prompt, temperature = 0.3, max_chars = 1500 } = input;

      const env = getEnv();
      if (!env.OPENROUTER_API_KEY?.trim()) {
        throw new AuthError("OpenRouter API key not configured", { source: "Text.generate" });
      }

      const model = getModelName();
      const baseURL = env.OPENROUTER_BASE_URL || "https://openrouter.ai/api/v1";

      debugTextGenerate(`Generating text with temperature ${temperature}, max ${max_chars} chars`);

      const systemPrompt = `You are a text generation assistant. Generate text based on the user's prompt.

If the prompt is malformed, unclear, inappropriate, or you cannot fulfill the request for any reason, provide brief reasoning and then output <ERROR>Short-error-text</ERROR> where Short-error-text explains the issue.`;

      try {
        const response = await fetch(`${baseURL}/chat/completions`, {
          method: "POST",
          headers: {
            Authorization: `Bearer ${env.OPENROUTER_API_KEY}`,
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            model,
            messages: [
              {
                role: "system",
                content: systemPrompt,
              },
              {
                role: "user",
                content: prompt,
              },
            ],
            temperature,
            max_tokens: Math.ceil(max_chars / 2), // Rough estimate: ~2 chars per token
            usage: {
              include: true,
            },
          }),
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw classifyHttpError(
            response.status,
            `OpenRouter API error: ${response.status} - ${errorText}`,
            { source: "Text.generate" }
          );
        }

        const result = await response.json();

        if (!result.choices || result.choices.length === 0) {
          throw new NetworkError("No response generated by the model", { source: "Text.generate" });
        }

        const usage = result.usage || {};
        const content = result.choices[0].message?.content?.trim();
        if (!content) {
          throw new LogicError("No content found in the response", { source: "Text.generate" });
        }

        // Check for error pattern
        const errorMatch = content.match(/<ERROR>(.*?)<\/ERROR>/s);
        if (errorMatch) {
          const errorMessage = errorMatch[1].trim();
          throw new LogicError(errorMessage || "LLM reported an error", { source: "Text.generate" });
        }

        debugTextGenerate("Generation completed", {
          promptLength: prompt.length,
          generatedLength: content.length
        }, "usage", usage);

        await getContext().createEvent("text_generate", {
          promptLength: prompt.length,
          generatedLength: content.length,
          temperature,
          maxChars: max_chars,
          ...formatUsageForEvent(usage),
        });

        return { text: content };
      } catch (error) {
        // Re-throw if already classified
        if (isClassifiedError(error)) {
          throw error;
        }
        throw classifyGenericError(error instanceof Error ? error : new Error(String(error)), "Text.generate");
      }
    },
  });
}
