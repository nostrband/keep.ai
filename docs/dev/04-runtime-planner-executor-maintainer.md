# Runtime Model: Planner, Executor, Maintainer

This document describes how Keep.AI runs automations while enforcing the core contract of **delegation over authorship**.

The runtime is intentionally split into three modes:

* **Planner** – generates implementations
* **Executor** – runs implementations
* **Maintainer** – proposes bounded repairs

Each mode has different responsibilities and capabilities.
Authority always remains in the **host runtime**, not in LLMs.

---

## Why a split runtime is required

A single agentic loop creates three failure modes:

1. **Probabilistic execution** - the same automation may behave differently on each run.

2. **Invisible ownership shifts** - when the agent “decides” at runtime, no one clearly owns correctness.

3. **Unbounded blast radius** - failures turn into creative improvisation instead of controlled handling.

To avoid this, Keep.AI enforces a strict separation:

> LLMs may write or repair code.
> They may not decide what to do at runtime.

All code generated by LLMs — whether during initial planning or during repair — is treated as **untrusted input** and must pass host-enforced validation before execution.

---

## High-level mode overview

| Mode       | Purpose                   | LLM/Agent | Authority |
| ---------- | ------------------------- | --------- | --------- |
| Planner    | Generate implementation   | Yes       | Host      |
| Executor   | Run implementation        | No        | Host      |
| Maintainer | Repair implementation     | Yes       | Host      |

---

## Planner

### Responsibility

The Planner translates **intent** into an **implementation**.

It runs when:

* a new automation is created
* intent is modified
* an implementation is missing or invalid

### Capabilities

The Planner may:

* inspect intent and constraints
* inspect prior runs and failures
* explore tools in a sandbox
* generate or update scripts and metadata

The Planner is allowed to be creative, exploratory and flexible.

### Constraints

The Planner:

* does not perform recurring execution
* does not perform uncontrolled side effects
* does not run without user involvement

The Planner output is treated as **untrusted input**, and is validated before deployment.

Validated output is a **script** - versioned code artifact suitable for execution.

Scripts versions have `major.minor` format, Planner always updates the major version and resets the minor version.

**Placeholder:**
Describe planner entry points, agent loop structure, and artifact storage.

---

## Executor

### Responsibility

The Executor runs the implementation produced by the Planner.

It is the **hot path** of the system.

### Guarantees

Execution is *deterministic in control flow*:

* a fixed, pre-generated implementation (JS code)
* no open-ended re-planning at runtime
* no agentic decision-making

Given the same inputs and external responses, the same steps are executed. 

> The *caveat* here is that usage of LLM primitives is possible inside an implementation (summarization, data extraction, etc), but such primitives are very constrained, validated and permissioned, allowing user to control the level of probabilistic behavior during runtime.

### Capabilities

The Executor:

* runs scripts inside a constrained sandbox
* invokes tools and connectors
* records logs, events, and outcomes
* emits success or failure signals

### Forbidden behavior

The Executor must never:

* call an LLM to drive it's behavior
* generate or modify code
* change behavior across runs of the same script version

If execution cannot proceed safely, it must stop.

---

### Sandbox and tool wrappers

Scripts run in a **fully sandboxed environment** with no direct access to external systems.

All external operations — network calls, database access, file I/O, API calls — are performed through **host-managed tool wrappers**. Scripts cannot bypass this layer.

Tool wrappers provide:

* **Permission enforcement** — every operation is checked against allowed permissions (see Chapter 11)
* **Mutation tracking** — side-effecting operations are recorded in the mutation ledger before execution (see Chapter 13)
* **Reconciliation** — uncertain outcomes are detected and resolved through tool-specific reconciliation methods (see Chapter 13)
* **Idempotency** — replay consults the ledger and returns cached results for applied mutations (see Chapter 14)
* **Payload comparison** — tools may provide `comparePayload` to assess replay divergence (see Chapter 14)
* **Logging** — all operations are recorded for observability and debugging

This architecture ensures that:

* the host runtime has complete visibility into script behavior
* no mutations can occur without durable tracking
* safety guarantees are enforced at the boundary, not trusted to script logic

**Placeholder:**
Document sandbox environment details, resource limits, and worker mapping.

---

## Maintainer

### Role and responsibility

The Maintainer is **not a decision-maker**.

It does not:

* classify failures
* choose recovery strategies
* decide whether escalation is required

Those decisions are made by the **host runtime**, using explicit policy.

The Maintainer is a **bounded repair capability**.

---

### Failure eligibility and gating

Before invoking the Maintainer, the host runtime deterministically verifies that:

* the failure is a repair-eligible logic error
* autonomous repair is permitted by policy
* safety and invariant constraints allow repair
* retry, cost, and time budgets are available

Failures that do not meet these conditions are never forwarded to the Maintainer.

**Placeholder:**
Describe failure taxonomy, eligibility rules, and gating logic.

---

### Repair responsibility

When invoked, the Maintainer’s sole responsibility is:

> Given intent, a failed implementation, and concrete failure evidence, propose a backward-compatible candidate fix that satisfies all constraints.

The Maintainer may:

* inspect the failed script
* inspect logs and run metadata
* generate a modified script or patch

The Maintainer may **not**:

* alter policy or permissions
* modify intent or relax it's constraints
* decide next steps

Its output is either a fixed script, or a failure that's escalated to user for re-planning.

Repaired scripts have their `minor` version number updated, the `major` version stays the same.

---

### Impact classification

In addition to proposing a fix, the Maintainer must classify the **potential impact** of the bug that was fixed:

* `safe` — the bug could not have affected external state (e.g., type errors, syntax issues, incorrect field access that would have failed before any mutation)
* `potentially_impactful` — the bug pattern could have caused incorrect mutations before failing (e.g., control flow errors, read-after-write dependencies, invalid logic)

The Maintainer is prompted to evaluate:

* Could incorrect mutations have executed before the failure occurred?
* Could control flow have diverged in a way that affected which mutations ran?
* Could the bug have caused duplicate, skipped, or incorrect mutations?

For `potentially_impactful` repairs:

* auto-fix activation is blocked pending user review
* the system shows items processed while the bug existed ("blast radius")
* user inspects affected items, verifies external state if needed, and approves activation

This classification does not detect actual damage — it surfaces uncertainty for user judgment. The Maintainer flags risk; the user decides whether to investigate or proceed.

This aligns with the delegation contract: when autonomous repair may have hidden consequences, the user is informed before resuming execution.

---

### Validation and deployment

All LLM code proposals (Planner/Maintainer) are validated by the host runtime:

* static checks (syntax, schema, invariants)
* sandboxed test or replay
* permission and capability checks
* budget enforcement
* LLM-reviewer/auditor calls

Only validated code is accepted and executed.

**Placeholder:**
Describe validation pipeline, replay harness, and versioning.

---

### Repair limits and escalation

Repair is explicitly bounded.

The host runtime enforces:

* maximum consecutive repair attempts
* maximum cumulative repair cost
* maximum time spent in repair

When limits are exceeded, autonomous recovery stops.

Escalation to user is **mandatory and deterministic**.

---

### Escalation to human action

When escalation is required, the runtime:

* pauses the automation
* records the failure state and evidence
* emits a structured “action needed” event

An LLM may be used **only** to:

* explain why execution cannot proceed
* summarize violated constraints
* suggest possible intent changes

LLM output does not unblock execution.

Only explicit human action can resume the automation.

**Placeholder:**
Map escalation to notification system and resume semantics.

---

## Mode transitions

Valid transitions:

* Planner → Executor (on user confirmation)
* Executor → Maintainer (on eligible failure)
* Maintainer → Executor (after validated repair)
* Maintainer → User (after failed repair)

Invalid transitions:

* Executor → Planner without failure and user action
* Executor → self-modifying execution
* Maintainer → escalation decision

**Placeholder:**
Describe state encoding and transition enforcement.

---

## Implications for contributors

Contributions must preserve these invariants:

* LLMs do not control execution
* policy lives in the host runtime
* repair is gated and bounded
* escalation is deterministic and auditable

Any change that increases LLM's ability to influence *whether* or *how* to proceed violates these invariants.

---

## Summary

The Planner creates implementations.
The Executor runs them.
The Maintainer proposes bounded repairs.

Authority always remains with the host runtime.

This separation is what makes delegated automation possible — and safe.

